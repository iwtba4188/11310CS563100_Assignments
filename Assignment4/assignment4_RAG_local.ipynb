{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJXPuMBgsO_I"
      },
      "source": [
        "# RAG using Langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J189g9PHscOJ"
      },
      "source": [
        "## Packages loading & import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LztGdoQClLDK"
      },
      "outputs": [],
      "source": [
        "# !pip install langchain\n",
        "# !pip install langchain_community\n",
        "# !pip install langchain_huggingface\n",
        "# !pip install langchain_text_splitters\n",
        "# !pip install langchain_chroma\n",
        "# !pip install rank-bm25\n",
        "# !pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "R7fVDp_MSM6d"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import json\n",
        "# import bs4\n",
        "import nltk\n",
        "# import torch\n",
        "import pickle\n",
        "# import numpy as np\n",
        "\n",
        "# from pyserini.index import IndexWriter\n",
        "# from pyserini.search import SimpleSearcher\n",
        "# from numpy.linalg import norm\n",
        "# from rank_bm25 import BM25Okapi\n",
        "# from nltk.tokenize import word_tokenize\n",
        "\n",
        "from langchain_community.llms import Ollama\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.vectorstores import Chroma\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "# from langchain_community.embeddings import JinaEmbeddings\n",
        "# from langchain.text_splitter import RecursiveCharacterTextSplitter, TokenTextSplitter\n",
        "from langchain.docstore.document import Document\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "# from langchain_community.document_loaders import WebBaseLoader\n",
        "# from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KZaXzYlMakAa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\X6959\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\X6959\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rqtezQ8shF0"
      },
      "source": [
        "## Hugging face login\n",
        "- Please apply the model first: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct\n",
        "- If you haven't been granted access to this model, you can use other LLM model that doesn't have to apply.\n",
        "- You must save the hf token otherwise you need to regenrate the token everytime.\n",
        "- When using Ollama, no login is required to access and utilize the llama model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Wr0_DNixrDT8"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "hf_token = \"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\"\n",
        "login(token=hf_token, add_to_git_credential=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zfN6G4fzr8CG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "�ϥΤ����r�X��: 950\n",
            "iwtba4188\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli whoami"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMtZo6OVsUP5"
      },
      "source": [
        "## TODO1: Set up the environment of Ollama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhZ_q0V4ciJc"
      },
      "source": [
        "### Introduction to Ollama\n",
        "- Ollama is a platform designed for running and managing large language models (LLMs) directly **on local devices**, providing a balance between performance, privacy, and control.\n",
        "- There are also other tools support users to manage LLM on local devices and accelerate it like *vllm*, *Llamafile*, *GPT4ALL*...etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xv8QM9xj3oUL"
      },
      "source": [
        "### Launch colabxterm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Y9agmoxFd7um"
      },
      "outputs": [],
      "source": [
        "# TODO1-1: You should install colab-xterm and launch it.\n",
        "# Write your commands here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVQMKksD6y4K",
        "outputId": "b07a3ee0-b0d6-453a-b49c-142c56e7c4ee"
      },
      "outputs": [],
      "source": [
        "# TODO1-2: You should install Ollama.\n",
        "# You may need root privileges if you use a local machine instead of Colab.\n",
        "# %xterm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "10NK09FKd_th"
      },
      "outputs": [],
      "source": [
        "# %xterm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jbj5MMnh8Htl"
      },
      "outputs": [],
      "source": [
        "# TODO1-3: Pull Llama3.2:1b via Ollama and start the Ollama service in the xterm\n",
        "# Write your commands in the xterm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22kvHBerCCqm"
      },
      "source": [
        "## Ollama testing\n",
        "You can test your Ollama status with the following cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "v_hBBGBOnH4P"
      },
      "outputs": [],
      "source": [
        "# Setting up the model that this tutorial will use\n",
        "MODEL = \"llama3.2:1b\" # https://ollama.com/library/llama3.2:3b\n",
        "EMBED_MODEL = \"jinaai/jina-embeddings-v2-base-en\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "T2sq6FoDWjVl"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\X6959\\AppData\\Local\\Temp\\ipykernel_6944\\2181764667.py:2: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
            "  llm = Ollama(model=MODEL)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The capital of Taiwan is Taipei.\n"
          ]
        }
      ],
      "source": [
        "# Initialize an instance of the Ollama model\n",
        "llm = Ollama(model=MODEL)\n",
        "# Invoke the model to generate responses\n",
        "response = llm.invoke(\"What is the capital of Taiwan?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5ndgmF6wYHY"
      },
      "source": [
        "## Build a simple RAG system by using LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lzul2yQR5q4I"
      },
      "source": [
        "### TODO2: Load the cat-facts dataset and prepare the retrieval database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "RJLNgILQ55UH"
      },
      "outputs": [],
      "source": [
        "# !wget https://huggingface.co/ngxson/demo_simple_rag_py/resolve/main/cat-facts.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "aMWMylYS-XRM"
      },
      "outputs": [],
      "source": [
        "# TODO2-1: Load the cat-facts dataset (as `refs`, which is a list of strings for all the cat facts)\n",
        "# Write your code here\n",
        "with open(\"cat-facts.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    refs = f.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "V66CXJr1BAu0"
      },
      "outputs": [],
      "source": [
        "# from langchain_core.documents import Document\n",
        "docs = [Document(page_content=doc, metadata={\"id\": i}) for i, doc in enumerate(refs)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "NmPvFlGxe-mr"
      },
      "outputs": [],
      "source": [
        "# Create an embedding model\n",
        "model_kwargs = {'trust_remote_code': True}\n",
        "encode_kwargs = {'normalize_embeddings': False}\n",
        "embeddings_model = HuggingFaceEmbeddings(\n",
        "    model_name=EMBED_MODEL,\n",
        "    model_kwargs=model_kwargs,\n",
        "    encode_kwargs=encode_kwargs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "A3Rn4z4siC6e"
      },
      "outputs": [],
      "source": [
        "# TODO2-2: Prepare the retrieval database\n",
        "# You should create a Chroma vector store.\n",
        "# search_type can be “similarity” (default), “mmr”, or “similarity_score_threshold”\n",
        "vector_store = Chroma.from_documents(\n",
        "    documents=docs, embedding=embeddings_model\n",
        ")\n",
        "retriever = vector_store.as_retriever(\n",
        "    search_type=\"mmr\", search_kwargs={\"k\": 3, \"fetch_k\": 5}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmtvJMdH61Y4"
      },
      "source": [
        "### Prompt setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "-oar4tEQONkl"
      },
      "outputs": [],
      "source": [
        "# TODO3: Set up the `system_prompt` and configure the prompt.\n",
        "system_prompt = (\n",
        "    # \"You are a cat expert. I will ask you some questions about cats. Please answer them to the best of your knowledge.\"  # shihtl> This prompt is generated by the GitHub Copilot.\n",
        "    # shihtl> Prompt 1: 10\n",
        "    # \"You must answer the questions based your given document, i.e., the original texts in the cat-facts dataset must appear in your answers.\"\n",
        "    # \"This texts cannot be modified or paraphrased, giving the reference from the given documents.\"\n",
        "    # \"For efficiency, you must answer the questions in a single sentence.\"  # shihtl> This prompt is generated by the GitHub Copilot.\n",
        "    # shihtl> Prompt 2: 8~9\n",
        "    # \"Answer the following questions about cats, in short, single sentence.\"\n",
        "    # \"Based on the given document, answer it without any modified or paraphrased, also giving reference in documents.\"\n",
        "    # shihtl> Prompt 3: 8~10\n",
        "    \"You must answer the questions in a single sentense.\"\n",
        "    \"The original texts in the cat-facts dataset must appear in your answers.\"\n",
        "    # shihtl> Prompt 4\n",
        "    # \"\"\n",
        "\n",
        "    # shihtl> Common part\n",
        "    \"Context: {context}\"\n",
        ")\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        # (\"human\", \"Answering the following questions about cats.\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rG9tEFr8wvLX"
      },
      "source": [
        "- For the vectorspace, the common algorithm would be used like Faiss, Chroma...(https://python.langchain.com/docs/integrations/vectorstores/) to deal with the extreme huge database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "yVbi3irCUQn9"
      },
      "outputs": [],
      "source": [
        "# TODO4: Build and run the RAG system\n",
        "# TODO4-1: Load the QA chain\n",
        "# You should create a chain for passing a list of Documents to a model.\n",
        "question_answer_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)\n",
        "\n",
        "# TODO4-2: Create retrieval chain\n",
        "# You should create retrieval chain that retrieves documents and then passes them on.\n",
        "chain = create_retrieval_chain(retriever=retriever, combine_docs_chain=question_answer_chain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "OLn0u3E-UwTK"
      },
      "outputs": [],
      "source": [
        "# Question (queries) and answer pairs\n",
        "# Please do not modify this cell.\n",
        "queries = [\n",
        "    \"How much of a day do cats spend sleeping on average?\",\n",
        "    \"What is the technical term for a cat's hairball?\",\n",
        "    \"What do scientists believe caused cats to lose their sweet tooth?\",\n",
        "    \"What is the top speed a cat can travel over short distances?\",\n",
        "    \"What is the name of the organ in a cat's mouth that helps it smell?\",\n",
        "    \"Which wildcat is considered the ancestor of all domestic cats?\",\n",
        "    \"What is the group term for cats?\",\n",
        "    \"How many different sounds can cats make?\",\n",
        "    \"What is the name of the first cat in space?\",\n",
        "    \"How many toes does a cat have on its back paws?\"\n",
        "]\n",
        "answers = [\n",
        "    \"2/3\",\n",
        "    \"Bezoar\",\n",
        "    \"a mutation in a key taste receptor\",\n",
        "    [\"31 mph\", \"49 km\"],\n",
        "    \"Jacobson’s organ\",\n",
        "    \"the African Wild Cat\",\n",
        "    \"clowder\",\n",
        "    \"100\",\n",
        "    [\"Felicette\", \"Astrocat\"],\n",
        "    \"four\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "6fLKCHWkizlr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: How much of a day do cats spend sleeping on average?\n",
            "Response: On average, a nine-year-old cat spends approximately one-third of its waking hours sleeping.\n",
            "\n",
            "Query: What is the technical term for a cat's hairball?\n",
            "Response: A bezoar, specifically a \"bezoar of fur\" or simply a hairbezoar.\n",
            "\n",
            "Query: What do scientists believe caused cats to lose their sweet tooth?\n",
            "Response: Scientists believe that a mutation in the taste receptor responsible for detecting sweetness is likely the reason why cats do not have a sweet tooth like dogs.\n",
            "\n",
            "Query: What is the top speed a cat can travel over short distances?\n",
            "Response: The top speed of a cat is approximately 31 miles per hour or around 49 kilometers per hour.\n",
            "\n",
            "Query: What is the name of the organ in a cat's mouth that helps it smell?\n",
            "Response: Cats have an additional organ called Jacobson’s organ located in the upper surface of their mouth.\n",
            "\n",
            "Query: Which wildcat is considered the ancestor of all domestic cats?\n",
            "Response: The ancestor of modern domestic cats, which still exists today, is believed by scientists to be the African Wild Cat.\n",
            "\n",
            "Query: What is the group term for cats?\n",
            "Response: A group of cats is commonly referred to as a \"clowder\".\n",
            "\n",
            "Query: How many different sounds can cats make?\n",
            "Response: Cats are capable of making approximately 100 different sounds, as recorded in the cat-facts dataset.\n",
            "\n",
            "Query: What is the name of the first cat in space?\n",
            "Response: Felicette, a French cat named \"Astrocat,\" was the first cat in space, launched by France in 1963.\n",
            "\n",
            "Query: How many toes does a cat have on its back paws?\n",
            "Response: Cats have five toes on each front paw, but only four toes on each back paw, which gives them up to 20 total toes on their back paws.\n",
            "\n",
            "Correct numbers: 7\n"
          ]
        }
      ],
      "source": [
        "counts = 0\n",
        "for i, query in enumerate(queries):\n",
        "    # TODO4-3: Run the RAG system\n",
        "    response = chain.invoke({\"input\": query})\n",
        "    # print(response)\n",
        "    print(f\"Query: {query}\\nResponse: {response['answer']}\\n\")\n",
        "    # The following lines perform evaluations.\n",
        "    # if the answer shows up in your response, the response is considered correct.\n",
        "    if type(answers[i]) == list:\n",
        "        for answer in answers[i]:\n",
        "            if answer.lower() in response[\"answer\"].lower():\n",
        "                counts += 1\n",
        "                break\n",
        "    else:\n",
        "        if answers[i].lower() in response[\"answer\"].lower():\n",
        "            counts += 1\n",
        "\n",
        "# TODO5: Improve to let the LLM correctly answer the ten questions.\n",
        "print(f\"Correct numbers: {counts}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Question (queries) and answer pairs\n",
        "# Please do not modify this cell.\n",
        "queries = [\n",
        "    \"How much of a day do cats spend sleeping on average?\",\n",
        "    \"What is the technical term for a cat's hairball?\",\n",
        "    \"What do scientists believe caused cats to lose their sweet tooth?\",\n",
        "    \"What is the top speed a cat can travel over short distances?\",\n",
        "    \"What is the name of the organ in a cat's mouth that helps it smell?\",\n",
        "    \"Which wildcat is considered the ancestor of all domestic cats?\",\n",
        "    \"What is the group term for cats?\",\n",
        "    \"How many different sounds can cats make?\",\n",
        "    \"What is the name of the first cat in space?\",\n",
        "    \"How many toes does a cat have on its back paws?\"\n",
        "]\n",
        "answers = [\n",
        "    \"2/3\",\n",
        "    \"Bezoar\",\n",
        "    \"a mutation in a key taste receptor\",\n",
        "    [\"31 mph\", \"49 km\"],\n",
        "    \"Jacobson’s organ\",\n",
        "    \"the African Wild Cat\",\n",
        "    \"clowder\",\n",
        "    \"100\",\n",
        "    [\"Felicette\", \"Astrocat\"],\n",
        "    \"four\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO3: Set up the `system_prompt` and configure the prompt.\n",
        "rules = [\n",
        "    \"You must answer the questions in a single sentense.\",\n",
        "    \"The original texts in the dataser must appear in your answers.\",\n",
        "    \"Answering without any modified or paraphrased the dataset.\",\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rule 1: You must answer the questions in a single sentense.\n",
            "Rule 2: The original texts in the dataser must appear in your answers.\n",
            "Rule 3: Answering without any modified or paraphrased the dataset.\n",
            "Rule 4: You must answer the questions in a single sentense.\n",
            "The original texts in the dataser must appear in your answers.\n",
            "Rule 5: You must answer the questions in a single sentense.\n",
            "Answering without any modified or paraphrased the dataset.\n",
            "Rule 6: The original texts in the dataser must appear in your answers.\n",
            "Answering without any modified or paraphrased the dataset.\n",
            "Rule 7: You must answer the questions in a single sentense.\n",
            "The original texts in the dataser must appear in your answers.\n",
            "Answering without any modified or paraphrased the dataset.\n"
          ]
        }
      ],
      "source": [
        "from itertools import combinations, chain\n",
        "\n",
        "for index, rule in enumerate(\n",
        "    chain.from_iterable(combinations(rules, r) for r in range(1, len(rules) + 1))\n",
        "):  # shihtl> This line of code is generated by the Microsoft Copilot.\n",
        "    print(f\"Rule {index + 1}: \" + \"\\n\".join(rule))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Question 1: 100%|██████████| 10/10 [00:39<00:00,  3.99s/it]orrect={0: [0, 1, 1, 1, 1, 1, 1, 1, 1, 0]}]\n",
            "Rule Combination 1: 100%|██████████| 1/1 [00:39<00:00, 39.87s/it, Correct={0: [0, 1, 1, 1, 1, 1, 1, 1, 1, 0]}]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Correct numbers: {0: [0, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Question 1:  70%|███████   | 7/10 [00:40<00:17,  5.84s/it]Correct={0: [0, 1, 1, 1, 1, 1, 1, 1, 1, 1], 1: [1, 1, 1, 1, 0, 1, 1, 0, 0, 0]}]\n",
            "Rule Combination 2:   0%|          | 0/1 [00:40<?, ?it/s, Correct={0: [0, 1, 1, 1, 1, 1, 1, 1, 1, 1], 1: [1, 1, 1, 1, 0, 1, 1, 0, 0, 0]}]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[86], line 35\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, query \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(query_bar):\n\u001b[0;32m     29\u001b[0m     round_bar\u001b[38;5;241m.\u001b[39mset_postfix(\n\u001b[0;32m     30\u001b[0m         {\n\u001b[0;32m     31\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCorrect\u001b[39m\u001b[38;5;124m\"\u001b[39m: statistics,\n\u001b[0;32m     32\u001b[0m         }\n\u001b[0;32m     33\u001b[0m     )\n\u001b[1;32m---> 35\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m# print(f\"Query: {query}\\nResponse: {response['answer']}\\n\")\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(answers[i]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n",
            "File \u001b[1;32mc:\\Tsung\\Programming\\GitHub\\11310CS563100_Assignments\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5354\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   5348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m   5349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5350\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[0;32m   5351\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   5352\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   5353\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[1;32m-> 5354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5355\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5356\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5357\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5358\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Tsung\\Programming\\GitHub\\11310CS563100_Assignments\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3024\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3022\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3023\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3024\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n\u001b[0;32m   3025\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   3026\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[1;32mc:\\Tsung\\Programming\\GitHub\\11310CS563100_Assignments\\venv\\Lib\\site-packages\\langchain_core\\runnables\\passthrough.py:494\u001b[0m, in \u001b[0;36mRunnableAssign.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    490\u001b[0m     \u001b[38;5;28minput\u001b[39m: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m    491\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    492\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    493\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m--> 494\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Tsung\\Programming\\GitHub\\11310CS563100_Assignments\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1927\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input, config, run_type, serialized, **kwargs)\u001b[0m\n\u001b[0;32m   1923\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[0;32m   1924\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[0;32m   1925\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[0;32m   1926\u001b[0m         Output,\n\u001b[1;32m-> 1927\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1928\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1929\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1930\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1931\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1932\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1933\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1934\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   1935\u001b[0m     )\n\u001b[0;32m   1936\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1937\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
            "File \u001b[1;32mc:\\Tsung\\Programming\\GitHub\\11310CS563100_Assignments\\venv\\Lib\\site-packages\\langchain_core\\runnables\\config.py:396\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    395\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[1;32m--> 396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Tsung\\Programming\\GitHub\\11310CS563100_Assignments\\venv\\Lib\\site-packages\\langchain_core\\runnables\\passthrough.py:481\u001b[0m, in \u001b[0;36mRunnableAssign._invoke\u001b[1;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_invoke\u001b[39m(\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28minput\u001b[39m: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    474\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    476\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mdict\u001b[39m\n\u001b[0;32m    477\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe input to RunnablePassthrough.assign() must be a dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    480\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m--> 481\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    486\u001b[0m     }\n",
            "File \u001b[1;32mc:\\Tsung\\Programming\\GitHub\\11310CS563100_Assignments\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3727\u001b[0m, in \u001b[0;36mRunnableParallel.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3722\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m   3723\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   3724\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(_invoke_step, step, \u001b[38;5;28minput\u001b[39m, config, key)\n\u001b[0;32m   3725\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   3726\u001b[0m         ]\n\u001b[1;32m-> 3727\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[43m{\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m   3728\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   3729\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[1;32mc:\\Tsung\\Programming\\GitHub\\11310CS563100_Assignments\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3727\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   3722\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m   3723\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   3724\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(_invoke_step, step, \u001b[38;5;28minput\u001b[39m, config, key)\n\u001b[0;32m   3725\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   3726\u001b[0m         ]\n\u001b[1;32m-> 3727\u001b[0m         output \u001b[38;5;241m=\u001b[39m {key: \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[0;32m   3728\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   3729\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m--> 451\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from itertools import combinations, chain\n",
        "\n",
        "\n",
        "statistics = {}\n",
        "\n",
        "for index, rule in enumerate(\n",
        "    chain.from_iterable(combinations(rules, r) for r in range(1, len(rules) + 1))\n",
        "):  # shihtl> This line of code is generated by the Microsoft Copilot.\n",
        "    statistics[index] = [0] * 10\n",
        "\n",
        "    system_prompt = \"\\n\".join(rule) + \"\\n\" + \"Context: {context}\"\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", system_prompt),\n",
        "            (\"human\", \"{input}\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    question_answer_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)\n",
        "    chain = create_retrieval_chain(\n",
        "        retriever=retriever, combine_docs_chain=question_answer_chain\n",
        "    )\n",
        "\n",
        "    round_num = 1\n",
        "    round_bar = tqdm(range(round_num), desc=f\"Rule Combination {index + 1}\")\n",
        "    for round in round_bar:\n",
        "        query_bar = tqdm(queries, desc=f\"Question {round + 1}\")\n",
        "        for i, query in enumerate(query_bar):\n",
        "            round_bar.set_postfix(\n",
        "                {\n",
        "                    \"Correct\": statistics,\n",
        "                }\n",
        "            )\n",
        "\n",
        "            response = chain.invoke({\"input\": query})\n",
        "            # print(f\"Query: {query}\\nResponse: {response['answer']}\\n\")\n",
        "\n",
        "            if type(answers[i]) == list:\n",
        "                for answer in answers[i]:\n",
        "                    if answer.lower() in response[\"answer\"].lower():\n",
        "                        statistics[index][i] += 1\n",
        "                        break\n",
        "            else:\n",
        "                if answers[i].lower() in response[\"answer\"].lower():\n",
        "                    statistics[index][i] += 1\n",
        "\n",
        "    print(f\"Correct numbers: {statistics}\")\n",
        "    with open(\"statistics.pickle\", \"wb\") as file:\n",
        "        pickle.dump(statistics, file)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
