{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10570,
     "status": "ok",
     "timestamp": 1732030046170,
     "user": {
      "displayName": "tl shih",
      "userId": "01416377648021197928"
     },
     "user_tz": -480
    },
    "id": "ODBX4uEwAXFb",
    "outputId": "2d1682f5-426e-4f58-8d86-0783a5b87a32"
   },
   "outputs": [],
   "source": [
    "# # shihtl> For Colab environment\n",
    "# !pip install torchmetrics\n",
    "# !pip install datasets==2.21.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2148,
     "status": "ok",
     "timestamp": 1732030048313,
     "user": {
      "displayName": "tl shih",
      "userId": "01416377648021197928"
     },
     "user_tz": -480
    },
    "id": "X-E6hUYmArHo",
    "outputId": "d60514e2-68d8-4161-b8c6-e8db3cb45415"
   },
   "outputs": [],
   "source": [
    "# # shihtl> For Colab environment\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 8560,
     "status": "ok",
     "timestamp": 1732030056870,
     "user": {
      "displayName": "tl shih",
      "userId": "01416377648021197928"
     },
     "user_tz": -480
    },
    "id": "g7VvJ9tLAUrR"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Tsung\\Programming\\GitHub\\11310CS563100_Assignments\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers as T\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from torchmetrics import SpearmanCorrCoef, Accuracy, F1Score\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import random\n",
    "import gc\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1732030056871,
     "user": {
      "displayName": "tl shih",
      "userId": "01416377648021197928"
     },
     "user_tz": -480
    },
    "id": "_18ZVmncAUrS"
   },
   "outputs": [],
   "source": [
    "# shihtl> Code ref: https://blog.csdn.net/u014687517/article/details/94719910\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "setup_seed(814750857)\n",
    "# shihtl> Section end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1732030056871,
     "user": {
      "displayName": "tl shih",
      "userId": "01416377648021197928"
     },
     "user_tz": -480
    },
    "id": "YrYUAK--AUrT"
   },
   "outputs": [],
   "source": [
    "# 有些中文的標點符號在tokenizer編碼以後會變成[UNK]，所以將其換成英文標點\n",
    "token_replacement = [\n",
    "    [\"：\" , \":\"],\n",
    "    [\"，\" , \",\"],\n",
    "    [\"“\" , \"\\\"\"],\n",
    "    [\"”\" , \"\\\"\"],\n",
    "    [\"？\" , \"?\"],\n",
    "    [\"……\" , \"...\"],\n",
    "    [\"！\" , \"!\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1494,
     "status": "ok",
     "timestamp": 1732030058361,
     "user": {
      "displayName": "tl shih",
      "userId": "01416377648021197928"
     },
     "user_tz": -480
    },
    "id": "jz0purqZAUrT",
    "outputId": "80148692-e403-4842-c853-a5b180e025fc"
   },
   "outputs": [],
   "source": [
    "model_name = \"google-bert/bert-base-uncased\"\n",
    "tokenizer = T.BertTokenizer.from_pretrained(model_name, cache_dir=\"./cache/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6140,
     "status": "ok",
     "timestamp": 1732030064499,
     "user": {
      "displayName": "tl shih",
      "userId": "01416377648021197928"
     },
     "user_tz": -480
    },
    "id": "poBDA4HHAUrU",
    "outputId": "aa2a9396-868f-4a54-decd-e7a92ba359f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset example: \n",
      "{'sentence_pair_id': 1, 'premise': 'A group of kids is playing in a yard and an old man is standing in the background', 'hypothesis': 'A group of boys in a yard is playing and a man is standing in the background', 'relatedness_score': 4.5, 'entailment_judgment': 0} \n",
      "{'sentence_pair_id': 2, 'premise': 'A group of children is playing in the house and there is no man standing in the background', 'hypothesis': 'A group of kids is playing in a yard and an old man is standing in the background', 'relatedness_score': 3.200000047683716, 'entailment_judgment': 0} \n",
      "{'sentence_pair_id': 3, 'premise': 'The young boys are playing outdoors and the man is smiling nearby', 'hypothesis': 'The kids are playing outdoors near a man with a smile', 'relatedness_score': 4.699999809265137, 'entailment_judgment': 1}\n",
      "Dataset example: \n",
      "{'sentence_pair_id': 4, 'premise': 'The young boys are playing outdoors and the man is smiling nearby', 'hypothesis': 'There is no boy playing outdoors and there is no man smiling', 'relatedness_score': 3.5999999046325684, 'entailment_judgment': 2} \n",
      "{'sentence_pair_id': 24, 'premise': 'A person in a black jacket is doing tricks on a motorbike', 'hypothesis': 'A skilled person is riding a bicycle on one wheel', 'relatedness_score': 3.4000000953674316, 'entailment_judgment': 0} \n",
      "{'sentence_pair_id': 105, 'premise': 'Four children are doing backbends in the gym', 'hypothesis': 'Four girls are doing backbends and playing outdoors', 'relatedness_score': 3.799999952316284, 'entailment_judgment': 0}\n",
      "Dataset example: \n",
      "{'sentence_pair_id': 6, 'premise': 'There is no boy playing outdoors and there is no man smiling', 'hypothesis': 'A group of kids is playing in a yard and an old man is standing in the background', 'relatedness_score': 3.299999952316284, 'entailment_judgment': 0} \n",
      "{'sentence_pair_id': 7, 'premise': 'A group of boys in a yard is playing and a man is standing in the background', 'hypothesis': 'The young boys are playing outdoors and the man is smiling nearby', 'relatedness_score': 3.700000047683716, 'entailment_judgment': 0} \n",
      "{'sentence_pair_id': 8, 'premise': 'A group of children is playing in the house and there is no man standing in the background', 'hypothesis': 'The young boys are playing outdoors and the man is smiling nearby', 'relatedness_score': 3.0, 'entailment_judgment': 0}\n"
     ]
    }
   ],
   "source": [
    "class SemevalDataset(Dataset):\n",
    "    def __init__(self, split=\"train\") -> None:\n",
    "        super().__init__()\n",
    "        assert split in [\"train\", \"validation\", \"test\"]\n",
    "        self.data = load_dataset(\n",
    "            \"sem_eval_2014_task_1\",\n",
    "            split=split,\n",
    "            cache_dir=\"./cache/\",\n",
    "            trust_remote_code=True,\n",
    "        ).to_list()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        d = self.data[index]\n",
    "        # 把中文標點替換掉\n",
    "        for k in [\"premise\", \"hypothesis\"]:\n",
    "            for tok in token_replacement:\n",
    "                d[k] = d[k].replace(tok[0], tok[1])\n",
    "        return d\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "data_sample = SemevalDataset(split=\"train\").data[:3]\n",
    "print(f\"Dataset example: \\n{data_sample[0]} \\n{data_sample[1]} \\n{data_sample[2]}\")\n",
    "data_sample = SemevalDataset(split=\"validation\").data[:3]\n",
    "print(f\"Dataset example: \\n{data_sample[0]} \\n{data_sample[1]} \\n{data_sample[2]}\")\n",
    "data_sample = SemevalDataset(split=\"test\").data[:3]\n",
    "print(f\"Dataset example: \\n{data_sample[0]} \\n{data_sample[1]} \\n{data_sample[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1732030064499,
     "user": {
      "displayName": "tl shih",
      "userId": "01416377648021197928"
     },
     "user_tz": -480
    },
    "id": "NVlDu_c-AUrU"
   },
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "lr = 3e-5\n",
    "epochs = 10\n",
    "train_batch_size = 8\n",
    "validation_batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5619,
     "status": "ok",
     "timestamp": 1732030070115,
     "user": {
      "displayName": "tl shih",
      "userId": "01416377648021197928"
     },
     "user_tz": -480
    },
    "id": "evH0hDj-AUrU",
    "outputId": "ae3146a8-6bf7-4405-f006-c4e2f1163248"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "({'input_ids': tensor([[  101,  1037,  2177,  1997,  4268,  2003,  2652,  1999,  1037,  4220,\n",
      "          1998,  2019,  2214,  2158,  2003,  3061,  1999,  1996,  4281,   102,\n",
      "          1037,  2177,  1997,  3337,  1999,  1037,  4220,  2003,  2652,  1998,\n",
      "          1037,  2158,  2003,  3061,  1999,  1996,  4281,   102,     0],\n",
      "        [  101,  1037,  2829,  3899,  2003,  7866,  2178,  4111,  1999,  2392,\n",
      "          1997,  1996,  2158,  1999,  6471,   102,  2048,  6077,  2024,  4843,\n",
      "          1998, 17662,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1996,  4268,  2024,  2652, 19350,  2379,  1037,  2158,  2007,\n",
      "          1037,  2868,   102,  1037,  2177,  1997,  4268,  2003,  2652,  1999,\n",
      "          1037,  4220,  1998,  2019,  2214,  2158,  2003,  3061,  1999,  1996,\n",
      "          4281,   102,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2048,  6077,  2024,  3554,   102,  2048,  6077,  2024,  4843,\n",
      "          1998, 17662,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1996,  2402,  3337,  2024,  2652, 19350,  1998,  1996,  2158,\n",
      "          2003,  5629,  3518,   102,  1996,  4268,  2024,  2652, 19350,  2379,\n",
      "          1037,  2158,  2007,  1037,  2868,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1037,  2829,  3899,  2003,  7866,  2178,  4111,  1999,  2392,\n",
      "          1997,  1996,  2158,  1999,  6471,   102,  2048,  6077,  2024,  3554,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1037,  2177,  1997,  2336,  2003,  2652,  1999,  1996,  2160,\n",
      "          1998,  2045,  2003,  2053,  2158,  3061,  1999,  1996,  4281,   102,\n",
      "          1037,  2177,  1997,  4268,  2003,  2652,  1999,  1037,  4220,  1998,\n",
      "          2019,  2214,  2158,  2003,  3061,  1999,  1996,  4281,   102],\n",
      "        [  101,  1996,  2402,  3337,  2024,  2652, 19350,  1998,  1996,  2158,\n",
      "          2003,  5629,  3518,   102,  1037,  2177,  1997,  4268,  2003,  2652,\n",
      "          1999,  1037,  4220,  1998,  2019,  2214,  2158,  2003,  3061,  1999,\n",
      "          1996,  4281,   102,     0,     0,     0,     0,     0,     0]],\n",
      "       device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]], device='cuda:0')}, tensor([4.5000, 3.2000, 3.4000, 4.0000, 4.7000, 3.5000, 3.2000, 3.7000],\n",
      "       device='cuda:0'), tensor([0, 0, 0, 0, 1, 0, 0, 0], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "# TODO1: Create batched data for DataLoader\n",
    "# `collate_fn` is a function that defines how the data batch should be packed.\n",
    "# This function will be called in the DataLoader to pack the data batch.\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # TODO1-1: Implement the collate_fn function\n",
    "    # Write your code here\n",
    "    # The input parameter is a data batch (tuple), and this function packs it into tensors.\n",
    "    # Use tokenizer to pack tokenize and pack the data and its corresponding labels.\n",
    "    # Return the data batch and labels for each sub-task.\n",
    "    premise_batch = [data[\"premise\"] for data in batch]\n",
    "    hypothesis_batch = [data[\"hypothesis\"] for data in batch]\n",
    "\n",
    "    batch_result = tokenizer(premise_batch, hypothesis_batch, truncation=True, padding=True)\n",
    "    for k, v in batch_result.items():\n",
    "        batch_result[k] = torch.tensor(v).to(device)\n",
    "\n",
    "    relatedness_score = torch.tensor([data[\"relatedness_score\"] for data in batch]).to(device)\n",
    "    entailment_judgment = torch.tensor([data[\"entailment_judgment\"] for data in batch]).to(device)\n",
    "\n",
    "    return batch_result, relatedness_score, entailment_judgment\n",
    "\n",
    "# TODO1-2: Define your DataLoader\n",
    "# dl_train = DataLoader(SemevalDataset(\"train\"), batch_size=train_batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "# dl_validation = DataLoader(SemevalDataset(\"validation\"), batch_size=validation_batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "# dl_test = DataLoader(SemevalDataset(\"test\"), batch_size=validation_batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "dl_train = DataLoader(SemevalDataset(\"train\").data[:8], batch_size=train_batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "dl_validation = DataLoader(SemevalDataset(\"validation\").data[:8], batch_size=validation_batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "dl_test = DataLoader(SemevalDataset(\"test\").data[:8], batch_size=validation_batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "res = next(iter(dl_train))\n",
    "print(res[0].keys())\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1732030070116,
     "user": {
      "displayName": "tl shih",
      "userId": "01416377648021197928"
     },
     "user_tz": -480
    },
    "id": "UoQQK_0qAUrV",
    "outputId": "bc6bca34-3bc0-4422-959b-6238954c64b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "102\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.vocab[\"[CLS]\"])\n",
    "print(tokenizer.vocab[\"[SEP]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1732030070116,
     "user": {
      "displayName": "tl shih",
      "userId": "01416377648021197928"
     },
     "user_tz": -480
    },
    "id": "7c4xDsxOAUrV",
    "outputId": "a8de4021-6c4b-4be1-dccb-a06b9e4aa053"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 1, 0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def logits_to_prediction(logits):\n",
    "    return np.argmax(logits.cpu(), axis=1)\n",
    "\n",
    "sample_logits = torch.tensor([[1, 2, 3], [0, 5, 1], [9, 0, 0]])\n",
    "logits_to_prediction(sample_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1732030070116,
     "user": {
      "displayName": "tl shih",
      "userId": "01416377648021197928"
     },
     "user_tz": -480
    },
    "id": "jMTRi438AUrW"
   },
   "outputs": [],
   "source": [
    "# shihtl> model1: vanilla model\n",
    "# TODO2: Construct your model\n",
    "class MultiLabelModel1(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Write your code here\n",
    "        # Define what modules you will use in the model\n",
    "        self.bert = T.AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name, cache_dir=\"./cache/\", output_hidden_states=True\n",
    "        )\n",
    "\n",
    "        self.regression = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=self.bert.config.hidden_size, out_features=1),\n",
    "        )  # shihtl> regression task\n",
    "\n",
    "        self.classificaton = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=self.bert.config.hidden_size, out_features=3),\n",
    "        )  # shihtl> classification task\n",
    "\n",
    "    def forward(self, batch_dict):\n",
    "        # Write your code here\n",
    "        # Forward pass\n",
    "        # shihtl> [CLS] label's hidden states\n",
    "        data = self.bert.forward(\n",
    "            input_ids=batch_dict[\"input_ids\"],\n",
    "            attention_mask=batch_dict[\"attention_mask\"],\n",
    "        ).hidden_states[-1][:, 0, :]\n",
    "        # shihtl> ans1.shape = (batch_size, 1), the regression result\n",
    "        ans1 = self.regression.forward(data)\n",
    "        # shihtl> ans2.shape = (batch_size, num_of_classes), the logits of classification\n",
    "        ans2 = self.classificaton.forward(data)\n",
    "\n",
    "        return ans1, ans2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1732030070116,
     "user": {
      "displayName": "tl shih",
      "userId": "01416377648021197928"
     },
     "user_tz": -480
    },
    "id": "b4ubWyDPAUrW"
   },
   "outputs": [],
   "source": [
    "# shihtl> model2: two linear layers model\n",
    "# TODO2: Construct your model\n",
    "class MultiLabelModel2(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Write your code here\n",
    "        # Define what modules you will use in the model\n",
    "        self.bert = T.AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name, cache_dir=\"./cache/\", output_hidden_states=True\n",
    "        )\n",
    "\n",
    "        self.regression = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=self.bert.config.hidden_size, out_features=256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=256, out_features=1),\n",
    "        )  # shihtl> regression task\n",
    "\n",
    "        self.classificaton = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=self.bert.config.hidden_size, out_features=256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=256, out_features=3),\n",
    "            torch.nn.Sigmoid(),\n",
    "        )  # shihtl> classification task\n",
    "\n",
    "    def forward(self, batch_dict):\n",
    "        # Write your code here\n",
    "        # Forward pass\n",
    "        # shihtl> [CLS] label's hidden states\n",
    "        data = self.bert.forward(\n",
    "            input_ids=batch_dict[\"input_ids\"],\n",
    "            attention_mask=batch_dict[\"attention_mask\"],\n",
    "        ).hidden_states[-1][:, 0, :]\n",
    "        # shihtl> ans1.shape = (batch_size, 1), the regression result\n",
    "        ans1 = self.regression.forward(data)\n",
    "        # shihtl> ans2.shape = (batch_size, num_of_classes), the logits of classification\n",
    "        ans2 = self.classificaton.forward(data)\n",
    "\n",
    "        return ans1, ans2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1732030070116,
     "user": {
      "displayName": "tl shih",
      "userId": "01416377648021197928"
     },
     "user_tz": -480
    },
    "id": "dmSeXPAoAUrW"
   },
   "outputs": [],
   "source": [
    "# shihtl> model3: model2 + dropout 0.1 model\n",
    "# TODO2: Construct your model\n",
    "class MultiLabelModel3(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Write your code here\n",
    "        # Define what modules you will use in the model\n",
    "        self.bert = T.AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name, cache_dir=\"./cache/\", output_hidden_states=True\n",
    "        )\n",
    "\n",
    "        self.regression = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(p=0.1),\n",
    "            torch.nn.Linear(in_features=self.bert.config.hidden_size, out_features=256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=256, out_features=1),\n",
    "        )  # shihtl> regression task\n",
    "\n",
    "        self.classificaton = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(p=0.1),\n",
    "            torch.nn.Linear(in_features=self.bert.config.hidden_size, out_features=256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=256, out_features=3),\n",
    "            torch.nn.Sigmoid(),\n",
    "        )  # shihtl> classification task\n",
    "\n",
    "    def forward(self, batch_dict):\n",
    "        # Write your code here\n",
    "        # Forward pass\n",
    "        # shihtl> [CLS] label's hidden states\n",
    "        data = self.bert.forward(\n",
    "            input_ids=batch_dict[\"input_ids\"],\n",
    "            attention_mask=batch_dict[\"attention_mask\"],\n",
    "        ).hidden_states[-1][:, 0, :]\n",
    "        # shihtl> ans1.shape = (batch_size, 1), the regression result\n",
    "        ans1 = self.regression.forward(data)\n",
    "        # shihtl> ans2.shape = (batch_size, num_of_classes), the logits of classification\n",
    "        ans2 = self.classificaton.forward(data)\n",
    "\n",
    "        return ans1, ans2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1732030070431,
     "user": {
      "displayName": "tl shih",
      "userId": "01416377648021197928"
     },
     "user_tz": -480
    },
    "id": "LoRT4Fv5AUrW"
   },
   "outputs": [],
   "source": [
    "# shihtl> model4: model3 + common layer model\n",
    "# TODO2: Construct your model\n",
    "class MultiLabelModel4(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Write your code here\n",
    "        # Define what modules you will use in the model\n",
    "        self.bert = T.AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name, cache_dir=\"./cache/\", output_hidden_states=True\n",
    "        )\n",
    "\n",
    "        self.common_layer = torch.nn.Linear(\n",
    "            in_features=self.bert.config.hidden_size,\n",
    "            out_features=self.bert.config.hidden_size,\n",
    "        )\n",
    "\n",
    "        self.regression = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(p=0.1),\n",
    "            torch.nn.Linear(in_features=self.bert.config.hidden_size, out_features=256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=256, out_features=1),\n",
    "        )  # shihtl> regression task\n",
    "\n",
    "        self.classificaton = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(p=0.1),\n",
    "            torch.nn.Linear(in_features=self.bert.config.hidden_size, out_features=256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=256, out_features=3),\n",
    "            torch.nn.Sigmoid(),\n",
    "        )  # shihtl> classification task\n",
    "\n",
    "    def forward(self, batch_dict):\n",
    "        # Write your code here\n",
    "        # Forward pass\n",
    "        # shihtl> [CLS] label's hidden states\n",
    "        data = self.bert.forward(\n",
    "            input_ids=batch_dict[\"input_ids\"],\n",
    "            attention_mask=batch_dict[\"attention_mask\"],\n",
    "        ).hidden_states[-1][:, 0, :]\n",
    "        # shihtl>\n",
    "        data = self.common_layer.forward(data)\n",
    "        # shihtl> ans1.shape = (batch_size, 1), the regression result\n",
    "        ans1 = self.regression.forward(data)\n",
    "        # shihtl> ans2.shape = (batch_size, num_of_classes), the logits of classification\n",
    "        ans2 = self.classificaton.forward(data)\n",
    "\n",
    "        return ans1, ans2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1732030070431,
     "user": {
      "displayName": "tl shih",
      "userId": "01416377648021197928"
     },
     "user_tz": -480
    },
    "id": "8miGs7CzAUrX"
   },
   "outputs": [],
   "source": [
    "# shihtl> model5: model4 + another common layer model\n",
    "# TODO2: Construct your model\n",
    "class MultiLabelModel5(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Write your code here\n",
    "        # Define what modules you will use in the model\n",
    "        self.bert = T.AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name, cache_dir=\"./cache/\", output_hidden_states=True\n",
    "        )\n",
    "\n",
    "        self.common_layer1 = torch.nn.Linear(\n",
    "            in_features=self.bert.config.hidden_size,\n",
    "            out_features=self.bert.config.hidden_size,\n",
    "        )\n",
    "        self.common_layer2 = torch.nn.Linear(\n",
    "            in_features=256,\n",
    "            out_features=256,\n",
    "        )\n",
    "\n",
    "        self.regression = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(p=0.1),\n",
    "            torch.nn.Linear(in_features=self.bert.config.hidden_size, out_features=256),\n",
    "            torch.nn.ReLU(),\n",
    "            self.common_layer2,\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=256, out_features=1),\n",
    "        )  # shihtl> regression task\n",
    "\n",
    "        self.classificaton = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(p=0.1),\n",
    "            torch.nn.Linear(in_features=self.bert.config.hidden_size, out_features=256),\n",
    "            torch.nn.ReLU(),\n",
    "            self.common_layer2,\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=256, out_features=3),\n",
    "            torch.nn.Sigmoid(),\n",
    "        )  # shihtl> classification task\n",
    "\n",
    "    def forward(self, batch_dict):\n",
    "        # Write your code here\n",
    "        # Forward pass\n",
    "        # shihtl> [CLS] label's hidden states\n",
    "        data = self.bert.forward(\n",
    "            input_ids=batch_dict[\"input_ids\"],\n",
    "            attention_mask=batch_dict[\"attention_mask\"],\n",
    "        ).hidden_states[-1][:, 0, :]\n",
    "        # shihtl>\n",
    "        data = self.common_layer1.forward(data)\n",
    "        # shihtl> ans1.shape = (batch_size, 1), the regression result\n",
    "        ans1 = self.regression.forward(data)\n",
    "        # shihtl> ans2.shape = (batch_size, num_of_classes), the logits of classification\n",
    "        ans2 = self.classificaton.forward(data)\n",
    "\n",
    "        return ans1, ans2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1732030070431,
     "user": {
      "displayName": "tl shih",
      "userId": "01416377648021197928"
     },
     "user_tz": -480
    },
    "id": "DBacXBeBAUrX"
   },
   "outputs": [],
   "source": [
    "# shihtl> model6: two bert model\n",
    "# TODO2: Construct your model\n",
    "class MultiLabelModel6(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Write your code here\n",
    "        # Define what modules you will use in the model\n",
    "        self.bert_reg = T.AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name, cache_dir=\"./cache/\", output_hidden_states=True\n",
    "        )\n",
    "        self.bert_hypo = T.AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name, cache_dir=\"./cache/\", output_hidden_states=True\n",
    "        )\n",
    "\n",
    "        self.regression = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=self.bert_reg.config.hidden_size, out_features=1),\n",
    "        )  # shihtl> regression task\n",
    "\n",
    "        self.classificaton = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=self.bert_hypo.config.hidden_size, out_features=3),\n",
    "        )  # shihtl> classification task\n",
    "\n",
    "    def forward(self, batch_dict):\n",
    "        # Write your code here\n",
    "        # Forward pass\n",
    "        # shihtl> [CLS] label's hidden states\n",
    "        data_reg = self.bert_reg.forward(\n",
    "            input_ids=batch_dict[\"input_ids\"],\n",
    "            attention_mask=batch_dict[\"attention_mask\"],\n",
    "        ).hidden_states[-1][:, 0, :]\n",
    "        data_hypo = self.bert_hypo.forward(\n",
    "            input_ids=batch_dict[\"input_ids\"],\n",
    "            attention_mask=batch_dict[\"attention_mask\"],\n",
    "        ).hidden_states[-1][:, 0, :]\n",
    "        # shihtl> ans1.shape = (batch_size, 1), the regression result\n",
    "        ans1 = self.regression.forward(data_reg)\n",
    "        # shihtl> ans2.shape = (batch_size, num_of_classes), the logits of classification\n",
    "        ans2 = self.classificaton.forward(data_hypo)\n",
    "\n",
    "        return ans1, ans2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 515
    },
    "executionInfo": {
     "elapsed": 34931,
     "status": "error",
     "timestamp": 1732030105360,
     "user": {
      "displayName": "tl shih",
      "userId": "01416377648021197928"
     },
     "user_tz": -480
    },
    "id": "ctU9k01TAUrX",
    "outputId": "210f1329-6b07-488f-f621-fb7ea5f000bb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Tsung\\Programming\\GitHub\\11310CS563100_Assignments\\venv\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: Metric `SpearmanCorrcoef` will save all targets and predictions in the buffer. For large datasets, this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================= MultiLabelModel1_ep0_lr3e-05_bs8 =================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch [1/10]:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8])\n",
      "torch.Size([8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch [1/10]:   0%|          | 0/1 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 131\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m model\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_class \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[0;32m    124\u001b[0m     MultiLabelModel1,\n\u001b[0;32m    125\u001b[0m     MultiLabelModel2,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    129\u001b[0m     MultiLabelModel6,\n\u001b[0;32m    130\u001b[0m ]:\n\u001b[1;32m--> 131\u001b[0m     \u001b[43mmodel_train_and_valid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;66;03m# shihtl> Prevent GPU out of memory.\u001b[39;00m\n\u001b[0;32m    134\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "Cell \u001b[1;32mIn[26], line 43\u001b[0m, in \u001b[0;36mmodel_train_and_valid\u001b[1;34m(model_class)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(output1\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(relatedness_score\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m premise_loss \u001b[38;5;241m=\u001b[39m criterion_premise(output1\u001b[38;5;241m.\u001b[39msqueeze(), relatedness_score)\n\u001b[0;32m     45\u001b[0m hypo_loss \u001b[38;5;241m=\u001b[39m criterion_hypo(output2, entailment_judgment)\n",
      "File \u001b[1;32mc:\\Tsung\\Programming\\GitHub\\11310CS563100_Assignments\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Tsung\\Programming\\GitHub\\11310CS563100_Assignments\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# shihtl> For report data collection\n",
    "records = {}\n",
    "\n",
    "def model_train_and_valid(model_class):\n",
    "    model = model_class().to(device)\n",
    "\n",
    "    # TODO3: Define your optimizer and loss function\n",
    "    # TODO3-1: Define your Optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    # TODO3-2: Define your loss functions (you should have two)\n",
    "    # Write your code here\n",
    "    criterion_premise = torch.nn.MSELoss()\n",
    "    criterion_hypo = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # scoring functions\n",
    "    spc = SpearmanCorrCoef().to(device)\n",
    "    acc = Accuracy(task=\"multiclass\", num_classes=3).to(device)\n",
    "    f1 = F1Score(task=\"multiclass\", num_classes=3, average=\"macro\").to(device)\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        PARAM = f\"{model.__class__.__name__}_ep{ep}_lr{lr}_bs{train_batch_size}\"\n",
    "        print(f\" {PARAM} \".center(100, \"=\"))\n",
    "        record = {}\n",
    "\n",
    "        loss_record_combined = []\n",
    "        loss_record_reg = []\n",
    "        loss_record_class = []\n",
    "\n",
    "        pbar = tqdm(dl_train)\n",
    "        pbar.set_description(f\"Training epoch [{ep+1}/{epochs}]\")\n",
    "        model.train()\n",
    "        # TODO4: Write the training loop\n",
    "        # Write your code here\n",
    "        # train your model\n",
    "        for batch_dict, relatedness_score, entailment_judgment in pbar:\n",
    "            # clear gradient\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass\n",
    "            output1, output2 = model(batch_dict)\n",
    "            # compute loss\n",
    "            print(output1.squeeze().shape)\n",
    "            print(relatedness_score.shape)\n",
    "            input()\n",
    "            premise_loss = criterion_premise(output1.squeeze(), relatedness_score)\n",
    "            hypo_loss = criterion_hypo(output2, entailment_judgment)\n",
    "            # back-propagation\n",
    "            loss = premise_loss + hypo_loss\n",
    "            loss.backward()\n",
    "            # model optimization\n",
    "            optimizer.step()\n",
    "\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "            loss_record_combined.append(loss.item())\n",
    "            loss_record_reg.append(premise_loss.item())\n",
    "            loss_record_class.append(hypo_loss.item())\n",
    "\n",
    "        # # shihtl> Draw training loss\n",
    "        # plt.figure(figsize=(4, 3))\n",
    "        # plt.xlabel(\"Epoch\")\n",
    "        # plt.ylabel(\"Training Loss (Combined)\")\n",
    "        # plt.plot(loss_record_combined)\n",
    "        # plt.show()\n",
    "\n",
    "        pbar = tqdm(dl_validation)\n",
    "        pbar.set_description(f\"Validation epoch [{ep+1}/{epochs}]\")\n",
    "        model.eval()\n",
    "        # TODO5: Write the evaluation loop\n",
    "        # Write your code here\n",
    "        # Evaluate your model\n",
    "        with torch.no_grad():\n",
    "            all_output1 = []\n",
    "            all_output2 = []\n",
    "            all_relatedness_score = []\n",
    "            all_entailment_judgment = []\n",
    "\n",
    "            for batch_dict, relatedness_score, entailment_judgment in pbar:\n",
    "                output1, output2 = model(batch_dict)\n",
    "\n",
    "                all_output1.append(output1)\n",
    "                all_output2.append(output2)\n",
    "                all_relatedness_score.append(relatedness_score)\n",
    "                all_entailment_judgment.append(entailment_judgment)\n",
    "\n",
    "        all_output1 = torch.vstack(all_output1)\n",
    "        all_output2 = torch.vstack(all_output2)\n",
    "        all_relatedness_score = torch.cat(all_relatedness_score)\n",
    "        all_entailment_judgment = torch.cat(all_entailment_judgment)\n",
    "\n",
    "        # Output all the evaluation scores (SpearmanCorrCoef, Accuracy, F1Score)\n",
    "        spearman = spc(all_output1.squeeze(), all_relatedness_score).item()\n",
    "        print(f\"Spearman Corr: {spearman}\")\n",
    "        accuracy = acc(all_output2, all_entailment_judgment).item()\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        f1_score = f1(all_output2, all_entailment_judgment).item()\n",
    "        print(f\"F1 Score: {f1_score}\\n\")\n",
    "\n",
    "        # shihtl> Get predict labels from logits\n",
    "        pred_value = logits_to_prediction(all_output2)\n",
    "        confusion = confusion_matrix(all_entailment_judgment.cpu(), pred_value.cpu())\n",
    "\n",
    "        # torch.save(model, f\"./saved_models/{PARAM}.ckpt\")\n",
    "\n",
    "        record[\"loss_record_combined\"] = loss_record_combined\n",
    "        record[\"loss_record_reg\"] = loss_record_reg\n",
    "        record[\"loss_record_class\"] = loss_record_class\n",
    "        record[\"spearman\"] = spearman\n",
    "        record[\"accuracy\"] = accuracy\n",
    "        record[\"f1_score\"] = f1_score\n",
    "        record[\"confusion\"] = confusion\n",
    "\n",
    "        records[PARAM] = record\n",
    "\n",
    "        # shihtl> This section of code was generated by GPT-4o in Assignment 2, and modified to fit Assignment 3.\n",
    "        # save the record\n",
    "        # with open(f\"./all_records.pkl\", \"wb\") as f:\n",
    "        with open(f\"drive/MyDrive/university_file/11310_NLP/all_records.pkl\", \"wb\") as f:\n",
    "            pickle.dump(records, f)\n",
    "        # shihtl> Section end.\n",
    "\n",
    "    model.cpu()\n",
    "    del model\n",
    "\n",
    "for model_class in [\n",
    "    MultiLabelModel1,\n",
    "    MultiLabelModel2,\n",
    "    MultiLabelModel3,\n",
    "    MultiLabelModel4,\n",
    "    MultiLabelModel5,\n",
    "    MultiLabelModel6,\n",
    "]:\n",
    "    model_train_and_valid(model_class)\n",
    "\n",
    "    # shihtl> Prevent GPU out of memory.\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Hs57z-HAUrX"
   },
   "source": [
    "For test set predictions, you can write perform evaluation simlar to #TODO5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "aborted",
     "timestamp": 1732030105361,
     "user": {
      "displayName": "tl shih",
      "userId": "01416377648021197928"
     },
     "user_tz": -480
    },
    "id": "iFsU8AywAUrY"
   },
   "outputs": [],
   "source": [
    "pbar = tqdm(dl_test)\n",
    "pbar.set_description(f\"Testing epoch [{ep+1}/{epochs}]\")\n",
    "model.eval()\n",
    "# Write your code here\n",
    "# Evaluate your model\n",
    "with torch.no_grad():\n",
    "    all_output1 = []\n",
    "    all_output2 = []\n",
    "    all_relatedness_score = []\n",
    "    all_entailment_judgment = []\n",
    "\n",
    "    for batch_dict, relatedness_score, entailment_judgment in pbar:\n",
    "        output1, output2 = model(batch_dict)\n",
    "\n",
    "        all_output1.append(output1)\n",
    "        all_output2.append(output2)\n",
    "        all_relatedness_score.append(relatedness_score)\n",
    "        all_entailment_judgment.append(entailment_judgment)\n",
    "\n",
    "    all_output1 = torch.vstack(all_output1)\n",
    "    all_output2 = torch.vstack(all_output2)\n",
    "    all_relatedness_score = torch.cat(all_relatedness_score)\n",
    "    all_entailment_judgment = torch.cat(all_entailment_judgment)\n",
    "\n",
    "    # Output all the evaluation scores (SpearmanCorrCoef, Accuracy, F1Score)\n",
    "    spearman = spc(all_output1.squeeze(), all_relatedness_score).item()\n",
    "    print(f\"Spearman Corr: {spearman}\")\n",
    "    accuracy = acc(all_output2, all_entailment_judgment).item()\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    f1_score = f1(all_output2, all_entailment_judgment).item()\n",
    "    print(f\"F1 Score: {f1_score}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shihtl> For report data collection\n",
    "records = {}\n",
    "\n",
    "def model_train_and_valid(model_class):\n",
    "    model = model_class().to(device)\n",
    "\n",
    "    # TODO3: Define your optimizer and loss function\n",
    "    # TODO3-1: Define your Optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    # TODO3-2: Define your loss functions (you should have two)\n",
    "    # Write your code here\n",
    "    criterion_premise = torch.nn.MSELoss()\n",
    "    criterion_hypo = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # scoring functions\n",
    "    spc = SpearmanCorrCoef().to(device)\n",
    "    acc = Accuracy(task=\"multiclass\", num_classes=3).to(device)\n",
    "    f1 = F1Score(task=\"multiclass\", num_classes=3, average=\"macro\").to(device)\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        PARAM = f\"{model.__class__.__name__}_ep{ep}_lr{lr}_bs{train_batch_size}\"\n",
    "        print(f\" {PARAM} \".center(100, \"=\"))\n",
    "        record = {}\n",
    "\n",
    "        loss_record_combined = []\n",
    "        loss_record_reg = []\n",
    "        loss_record_class = []\n",
    "\n",
    "        pbar = tqdm(dl_train)\n",
    "        pbar.set_description(f\"Training epoch [{ep+1}/{epochs}]\")\n",
    "        model.train()\n",
    "        # TODO4: Write the training loop\n",
    "        # Write your code here\n",
    "        # train your model\n",
    "        for batch_dict, relatedness_score, entailment_judgment in pbar:\n",
    "            # clear gradient\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass\n",
    "            output1, output2 = model(batch_dict)\n",
    "            # compute loss\n",
    "            premise_loss = criterion_premise(output1, relatedness_score)\n",
    "            hypo_loss = criterion_hypo(output2, entailment_judgment)\n",
    "            # back-propagation\n",
    "            loss = premise_loss + hypo_loss\n",
    "            loss.backward()\n",
    "            # model optimization\n",
    "            optimizer.step()\n",
    "\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "            loss_record_combined.append(loss.item())\n",
    "            loss_record_reg.append(premise_loss.item())\n",
    "            loss_record_class.append(hypo_loss.item())\n",
    "\n",
    "        # # shihtl> Draw training loss\n",
    "        # plt.figure(figsize=(4, 3))\n",
    "        # plt.xlabel(\"Epoch\")\n",
    "        # plt.ylabel(\"Training Loss (Combined)\")\n",
    "        # plt.plot(loss_record_combined)\n",
    "        # plt.show()\n",
    "\n",
    "        pbar = tqdm(dl_validation)\n",
    "        pbar.set_description(f\"Validation epoch [{ep+1}/{epochs}]\")\n",
    "        model.eval()\n",
    "        # TODO5: Write the evaluation loop\n",
    "        # Write your code here\n",
    "        # Evaluate your model\n",
    "        with torch.no_grad():\n",
    "            all_output1 = []\n",
    "            all_output2 = []\n",
    "            all_relatedness_score = []\n",
    "            all_entailment_judgment = []\n",
    "\n",
    "            for batch_dict, relatedness_score, entailment_judgment in pbar:\n",
    "                output1, output2 = model(batch_dict)\n",
    "\n",
    "                all_output1.append(output1)\n",
    "                all_output2.append(output2)\n",
    "                all_relatedness_score.append(relatedness_score)\n",
    "                all_entailment_judgment.append(entailment_judgment)\n",
    "\n",
    "        all_output1 = torch.vstack(all_output1)\n",
    "        all_output2 = torch.vstack(all_output2)\n",
    "        all_relatedness_score = torch.cat(all_relatedness_score)\n",
    "        all_entailment_judgment = torch.cat(all_entailment_judgment)\n",
    "\n",
    "        # Output all the evaluation scores (SpearmanCorrCoef, Accuracy, F1Score)\n",
    "        spearman = spc(all_output1.squeeze(), all_relatedness_score).item()\n",
    "        print(f\"Spearman Corr: {spearman}\")\n",
    "        accuracy = acc(all_output2, all_entailment_judgment).item()\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        f1_score = f1(all_output2, all_entailment_judgment).item()\n",
    "        print(f\"F1 Score: {f1_score}\\n\")\n",
    "\n",
    "        # shihtl> Get predict labels from logits\n",
    "        pred_value = logits_to_prediction(all_output2)\n",
    "        confusion = confusion_matrix(all_entailment_judgment.cpu(), pred_value.cpu())\n",
    "\n",
    "        # torch.save(model, f\"./saved_models/{PARAM}.ckpt\")\n",
    "\n",
    "        record[\"loss_record_combined\"] = loss_record_combined\n",
    "        record[\"loss_record_reg\"] = loss_record_reg\n",
    "        record[\"loss_record_class\"] = loss_record_class\n",
    "        record[\"spearman\"] = spearman\n",
    "        record[\"accuracy\"] = accuracy\n",
    "        record[\"f1_score\"] = f1_score\n",
    "        record[\"confusion\"] = confusion\n",
    "\n",
    "        records[PARAM] = record\n",
    "\n",
    "        # shihtl> This section of code was generated by GPT-4o in Assignment 2, and modified to fit Assignment 3.\n",
    "        # save the record\n",
    "        # with open(f\"./all_records.pkl\", \"wb\") as f:\n",
    "        with open(f\"drive/MyDrive/university_files/11310_NLP/all_records.pkl\", \"wb\") as f:\n",
    "            pickle.dump(records, f)\n",
    "        # shihtl> Section end.\n",
    "\n",
    "    # shihtl> Testing\n",
    "    record = {}\n",
    "    PARAM = f\"{model.__class__.__name__}_lr{lr}_bs{train_batch_size}\"\n",
    "    print(f\" [TEST] {PARAM} \".center(100, \"=\"))\n",
    "    pbar = tqdm(dl_test)\n",
    "    pbar.set_description(f\"Testing epoch [{ep+1}/{epochs}]\")\n",
    "    model.eval()\n",
    "    # Write your code here\n",
    "    # Evaluate your model\n",
    "    with torch.no_grad():\n",
    "        all_output1 = []\n",
    "        all_output2 = []\n",
    "        all_relatedness_score = []\n",
    "        all_entailment_judgment = []\n",
    "\n",
    "        for batch_dict, relatedness_score, entailment_judgment in pbar:\n",
    "            output1, output2 = model(batch_dict)\n",
    "\n",
    "            all_output1.append(output1)\n",
    "            all_output2.append(output2)\n",
    "            all_relatedness_score.append(relatedness_score)\n",
    "            all_entailment_judgment.append(entailment_judgment)\n",
    "\n",
    "    all_output1 = torch.vstack(all_output1)\n",
    "    all_output2 = torch.vstack(all_output2)\n",
    "    all_relatedness_score = torch.cat(all_relatedness_score)\n",
    "    all_entailment_judgment = torch.cat(all_entailment_judgment)\n",
    "\n",
    "    # Output all the evaluation scores (SpearmanCorrCoef, Accuracy, F1Score)\n",
    "    spearman = spc(all_output1.squeeze(), all_relatedness_score).item()\n",
    "    print(f\"Spearman Corr: {spearman}\")\n",
    "    accuracy = acc(all_output2, all_entailment_judgment).item()\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    f1_score = f1(all_output2, all_entailment_judgment).item()\n",
    "    print(f\"F1 Score: {f1_score}\\n\")\n",
    "\n",
    "    # shihtl> Get predict labels from logits\n",
    "    pred_value = logits_to_prediction(all_output2)\n",
    "    confusion = confusion_matrix(all_entailment_judgment.cpu(), pred_value.cpu())\n",
    "\n",
    "    torch.save(model, f\"drive/MyDrive/university_files/11310_NLP/{PARAM}.ckpt\")\n",
    "\n",
    "    record[\"loss_record_combined\"] = loss_record_combined\n",
    "    record[\"loss_record_reg\"] = loss_record_reg\n",
    "    record[\"loss_record_class\"] = loss_record_class\n",
    "    record[\"spearman\"] = spearman\n",
    "    record[\"accuracy\"] = accuracy\n",
    "    record[\"f1_score\"] = f1_score\n",
    "    record[\"confusion\"] = confusion\n",
    "\n",
    "    records[PARAM] = record\n",
    "\n",
    "    # shihtl> This section of code was generated by GPT-4o in Assignment 2, and modified to fit Assignment 3.\n",
    "    # save the record\n",
    "    # with open(f\"./all_records.pkl\", \"wb\") as f:\n",
    "    with open(f\"drive/MyDrive/university_files/11310_NLP/all_records.pkl\", \"wb\") as f:\n",
    "        pickle.dump(records, f)\n",
    "    # shihtl> Section end.\n",
    "    # shihtl> Testing end.\n",
    "\n",
    "    model.cpu()\n",
    "    del model\n",
    "\n",
    "for model_class in [\n",
    "    MultiLabelModel1,\n",
    "    MultiLabelModel2,\n",
    "    MultiLabelModel3,\n",
    "    MultiLabelModel4,\n",
    "    MultiLabelModel5,\n",
    "    MultiLabelModel6,\n",
    "]:\n",
    "    model_train_and_valid(model_class)\n",
    "\n",
    "    # shihtl> Prevent GPU out of memory.\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8., dtype=torch.float64)\n",
      "tensor(4.6667, dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Tsung\\Programming\\GitHub\\11310CS563100_Assignments\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([3, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "testttt = torch.nn.MSELoss()\n",
    "print(testttt(torch.tensor([[4], [5], [9]], dtype=float), torch.tensor([3, 7, 6], dtype=float)))\n",
    "print(testttt(torch.tensor([4, 5, 9], dtype=float), torch.tensor([3, 7, 6], dtype=float)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
